{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
      ],
      "metadata": {
        "id": "QgIsnPVKlHim"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "98QB1WO9lNgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers peft datasets accelerate torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TbOI0K0Co3Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kmDfqaTEpL_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"{q} {a}\" for q, a in zip(examples[\"input\"], examples[\"output\"])]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Prepare labels as well (outputs)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"output\"], max_length=512, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Add labels to the tokenized inputs\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "cBPiRFtIphVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=1,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    base_model_name_or_path=\"microsoft/Phi-3.5-mini-instruct\",\n",
        "    target_modules=[\n",
        "        \"self_attn.qkv_proj\",        # Query, key, value projections in attention\n",
        "        \"self_attn.o_proj\",          # Output projection in attention\n",
        "        \"mlp.gate_up_proj\",          # MLP up-projection\n",
        "        \"mlp.down_proj\"              # MLP down-projection\n",
        "    ]\n",
        ")\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "UwJqxIJdswcA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    report_to='none',\n",
        "    logging_dir='./logs',\n",
        "    save_total_limit=2\n",
        ")"
      ],
      "metadata": {
        "id": "bm3aPqgcy4am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1)  # 10% for test\n",
        "print(tokenized_dataset.keys())   9\n",
        "\n",
        "# Proceed with Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"]\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XlWcJnX9y6U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "P6kAbIF7zw0c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}